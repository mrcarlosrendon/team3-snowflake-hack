{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "e7z5wrki7zen2xnabrhu",
   "authorId": "468490634528",
   "authorName": "USER",
   "authorEmail": "becky.oconnor@snowflake.com",
   "sessionId": "383eaec9-e238-4f73-9134-79e272531d46",
   "lastEditTime": 1748011257671
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aff1ade-51ff-4a8c-af54-af126b59f72e",
   "metadata": {
    "collapsed": false,
    "name": "Title"
   },
   "source": [
    "## Geospatial Hackathon Example\n",
    "#### Populations effected by Hurricane Ida\n",
    "\n",
    "This is an example of how you could use 3 of the tables provided to understand who is affected by hurricane IDA.  We will start by importing the basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pydeck as pdk \n",
    "import json\n",
    "\n",
    "\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark.types import *\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e6b3f-9883-44d8-b53b-8943a43878ae",
   "metadata": {
    "collapsed": false,
    "name": "head_hurricane_points"
   },
   "source": [
    "Here is a dataset viewing all hurricane points from hurricane IDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdc31a-a09e-4ee8-acc2-1bee75330ecc",
   "metadata": {
    "language": "sql",
    "name": "view_hurricane_points"
   },
   "outputs": [],
   "source": [
    "select * from DATAOPS_EVENT_PROD.HACKATHON_DATASETS.HURRICANE_POINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea860c-e061-4a10-8185-4ae84c6c505e",
   "metadata": {
    "collapsed": false,
    "name": "head_headmap"
   },
   "source": [
    "Lets visualise the points using a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ed5ba-7c75-4ca2-8a9d-ba82860ccb6e",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "view_heatmap"
   },
   "outputs": [],
   "source": [
    "tooltip = {\n",
    "   \"html\": \"\"\"<b>Name:</b> {NAME} <br> <b>USA Wind:</b> {USA_WIND} <br> <b>Hurricane Date:</b> {HURRICANE_DATE}\"\"\",\n",
    "   \"style\": {\n",
    "       \"width\":\"50%\",\n",
    "        \"backgroundColor\": \"steelblue\",\n",
    "        \"color\": \"white\",\n",
    "       \"text-wrap\": \"balance\"\n",
    "   }\n",
    "}\n",
    "\n",
    "hurricane_points = session.table('HACKATHON_DATASETS.HURRICANE_POINTS')\n",
    "    \n",
    "\n",
    "hurricane_pointspd = hurricane_points.to_pandas()\n",
    "center = hurricane_points.agg(avg('LAT'),avg('LON'))\n",
    "\n",
    "LAT = center.collect()[0][0]\n",
    "LON = center.collect()[0][1]\n",
    "\n",
    "\n",
    "h_points = pdk.Layer(\n",
    "            'HeatmapLayer',\n",
    "            data=hurricane_pointspd,\n",
    "            get_position=['LON','LAT'],\n",
    "            get_color='[41,181,232]',\n",
    "            get_radius=10,\n",
    "            pickable=True)\n",
    "\n",
    "map = pdk.Deck(\n",
    "    \n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=800,\n",
    "        \n",
    "        ),\n",
    "\n",
    "    layers= [h_points],tooltip=tooltip,\n",
    "    map_style=None\n",
    "    \n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "st.pydeck_chart(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fea611-446e-470a-a0cc-1534387adb59",
   "metadata": {
    "collapsed": false,
    "name": "head_hurricane_tracks"
   },
   "source": [
    "Take a look at the next dataset - these are the track lines of the hurricane.  Below, we are also visualising the linestrings.  The tracks are filtered on two states -  **Mississippi** and **Louisana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360a78a-d1bb-4b76-9565-ff584d0973c5",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "hurricane_track_lines"
   },
   "outputs": [],
   "source": [
    "tooltip = {\n",
    "   \"html\": \"\"\"<b>Name:</b> {NAME} <br> <b>USA Wind:</b> {USA_WIND} <br> <b>Hurricane Date:</b> {HURRICANE_DATE}\"\"\",\n",
    "   \"style\": {\n",
    "       \"width\":\"50%\",\n",
    "        \"backgroundColor\": \"steelblue\",\n",
    "        \"color\": \"white\",\n",
    "       \"text-wrap\": \"balance\"\n",
    "   }\n",
    "}\n",
    "\n",
    "hurricane_tracks = session.table('HACKATHON_DATASETS.HURRICANE_TRACKS')\n",
    "        #.with_column('LON',round('LON',2).astype(FloatType()))\\\n",
    "    #.with_column('LAT',round('LAT',2).astype(FloatType()))\n",
    "\n",
    "hurricane_trackspd = hurricane_tracks.to_pandas()\n",
    "hurricane_trackspd[\"GEO\"] = hurricane_trackspd[\"GEO\"].apply(lambda row: json.loads(row)[\"coordinates\"])\n",
    "\n",
    "st.write(hurricane_tracks)\n",
    "center = hurricane_tracks.agg(avg('LAT'),avg('LON'))\n",
    "\n",
    "LAT = center.collect()[0][0]\n",
    "LON = center.collect()[0][1]\n",
    "\n",
    "\n",
    "tracks_layer  = pdk.Layer(\n",
    "        type=\"PathLayer\",\n",
    "        data=hurricane_trackspd,\n",
    "        pickable=True,\n",
    "        get_color=[170, 74, 68],\n",
    "        width_scale=5,\n",
    "        opacity = 1,\n",
    "        width_min_pixels=5,\n",
    "        get_path=\"GEO\",\n",
    "        get_width=200,\n",
    ")\n",
    "\n",
    "map = pdk.Deck(\n",
    "    \n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=800,\n",
    "        \n",
    "        ),\n",
    "\n",
    "    layers= [tracks_layer],tooltip=tooltip,\n",
    "    map_style=None\n",
    "    \n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "st.pydeck_chart(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d3f11-c37b-416e-9f6e-95c8ec1542fd",
   "metadata": {
    "collapsed": false,
    "name": "head_population_census"
   },
   "source": [
    "You will now bring in the Population Census Blocks for **Louisiana** and **Mississippi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317467e-8a9c-4fa6-8236-5978b3ccdf0a",
   "metadata": {
    "language": "python",
    "name": "population_census_block"
   },
   "outputs": [],
   "source": [
    "population = session.table('DATAOPS_EVENT_PROD.HACKATHON_DATASETS.POPULATION_CENSUS_BLOCK')\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f51c185-b004-43c6-9837-10ed80d1e44e",
   "metadata": {
    "collapsed": false,
    "name": "multi_polygons"
   },
   "source": [
    "The census blocks are multi polygons - we will transform to polygons to use in **Pydeck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf50e209-d97f-4611-a169-6c7cb79b8388",
   "metadata": {
    "language": "python",
    "name": "flatten_polygons"
   },
   "outputs": [],
   "source": [
    "popf = population.join_table_function('flatten',\n",
    "                                        call_function('ST_ASGEOJSON',\n",
    "                                        col('GEO'))['coordinates']).drop('SEQ',\n",
    "                                                                               'KEY',\n",
    "                                                                               'PATH',\n",
    "                                                                               'INDEX',\n",
    "                                                                               'THIS')   \n",
    "popf = popf.with_column('GEO',\n",
    "                                to_geography(object_construct(lit('coordinates'),\n",
    "                                                        to_array('VALUE'),\n",
    "                                                        lit('type'),\n",
    "                                                        lit('Polygon')))).drop('VALUE')\n",
    "popf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83806d-a96c-455d-930e-849727eec196",
   "metadata": {
    "collapsed": false,
    "name": "join_track_with_population_blocks"
   },
   "source": [
    "Here we are going to join the population dataset with the hurricane tracks.  We will use the intersects to do this.  To avoid overlapping, we filter out duplicates due to one line intersecting multiple population blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08514db5-67db-420b-99af-4d6e0ad75cdf",
   "metadata": {
    "language": "python",
    "name": "join_intersects"
   },
   "outputs": [],
   "source": [
    "data = popf.join(hurricane_tracks,call_function('ST_INTERSECTS',hurricane_tracks['GEO'],popf['GEO']),rsuffix='_hurricane_track')\n",
    "\n",
    "### remove duplicates caused by ST_INTERSECTS - prioritise ones with largest housing units\n",
    "window_spec = Window.partition_by(popf['OBJECTID']).order_by(col('P0050014').asc())\n",
    "data = data.with_column('row_num', row_number().over(window_spec))#.qualify(col('row_num') == 1).drop('row_num')\n",
    "data = data.filter(col('row_num') == 1).drop('row_num')\n",
    "\n",
    "### simplify the polygons to reduce memory load in streamlit\n",
    "data = data.with_column('GEO',call_function('ST_SIMPLIFY',col('GEO'),10))\n",
    "\n",
    "### extract out coordinates to use with pydeck\n",
    "data = data.with_column('COORDINATES',call_function('ST_ASGEOJSON',col('GEO'))['coordinates'])\n",
    "data = data.cache_result()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bee20-e1d9-43b4-8fd7-2d87609a9c4e",
   "metadata": {
    "collapsed": false,
    "name": "head_populations_affected"
   },
   "source": [
    "Below is a map that shows all the polygons that were effected by the hurricane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bea55-eb15-42f4-a0c0-6561906c192d",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "view_all_affected_areas"
   },
   "outputs": [],
   "source": "import json\nimport streamlit as st\nimport pandas as pd\nimport pydeck as pdk\nimport json\nimport warnings\nimport altair as alt\n\nwarnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\nsession = get_active_session()\nfrom snowflake.snowpark.functions import *\nst.subheader('Affected Populations by the Hurricane')\n### create a filter dropdown using distinct urban extent values\n\n### create a center point - this time using the centroid method as we are visualising one polygon at a time\ncentre = data.with_column('CENTROID',call_function('ST_CENTROID',col('GEO')))\ncentre = centre.with_column('LON',call_function('ST_X',col('CENTROID')))\ncentre = centre.with_column('LAT',call_function('ST_Y',col('CENTROID')))\n\ncentrepd = centre.select('LON','LAT').to_pandas()\nLON = centrepd.LON.iloc[0]\nLAT = centrepd.LAT.iloc[0]\n\n### drop geo as we will be using coordinates field\ndata1 = data.drop('GEO')\ndata1 = data1.dropna()\ndatapd = data.to_pandas()\n\n### minmax populationss for \nmin_pop = datapd['P0030007'].min()\nmax_pop = datapd['P0030007'].max()\n\n# Define the start and end colors for the gradient (RGBA format)\n# Low population (min_pop) will be BLUE\nCOLOR_LOW_POP = [41, 181, 232, 255] # Blue (R, G, B, A)\n# High population (max_pop) will be ORANGE\nCOLOR_HIGH_POP = [255, 159, 54, 255] # Orange (R, G, B, A)\n\ndef get_color_from_population(population):\n    # Handle the edge case where all population values are the same\n    if min_pop == max_pop:\n        # If no variation, assign a mid-point color (or one of the extremes)\n        return COLOR_HIGH_POP # Or COLOR_LOW_POP, or an average\n    else:\n        # Normalize population to a 0-1 range\n        normalized_pop = (population - min_pop) / (max_pop - min_pop)\n\n        # Interpolate between the start and end colors\n        r = int(COLOR_LOW_POP[0] * (1 - normalized_pop) + COLOR_HIGH_POP[0] * normalized_pop)\n        g = int(COLOR_LOW_POP[1] * (1 - normalized_pop) + COLOR_HIGH_POP[1] * normalized_pop)\n        b = int(COLOR_LOW_POP[2] * (1 - normalized_pop) + COLOR_HIGH_POP[2] * normalized_pop)\n        a = int(COLOR_LOW_POP[3] * (1 - normalized_pop) + COLOR_HIGH_POP[3] * normalized_pop) # Interpolate alpha too if needed, or keep fixed\n\n        return [r, g, b, a]\n\ndatapd['fill_color'] = datapd['P0030007'].apply(get_color_from_population)\n\ntooltip = {\n   \"html\": \"\"\"<b>Name:</b> {NAME} \n                <br> <b>USA Wind:</b> {USA_WIND} \n                <br> <b>Water Surfaces Sq Feet:</b> {AWATER}\n                <br> <b>Populated Density:</b> {P001_CALC_PCTPOPDENSITY}\n                <br> <b>Population County of Housing Units: </b> {P0020003}\n                <br> <b>Population Count: </b> {P0030007}\"\"\",\n   \"style\": {\n       \"width\":\"50%\",\n        \"backgroundColor\": \"steelblue\",\n        \"color\": \"white\",\n       \"text-wrap\": \"balance\"\n   }\n}\n\n\n\n# convert the dataframe to pandas and use a pandas lamda function to extract the coordinates out of each polygon.  \n##pydeck only requires sets of coordinates in arrays, not the polygon itself\n\n\ndatapd[\"COORDINATES\"] = datapd[\"COORDINATES\"].apply(lambda row: json.loads(row))\n\ncol1, col2,col3,col4 = st.columns(4)\nwith col1:\n    bar_data = data1.group_by('COUNTY')\\\n        .agg(avg('USA_WIND').alias('USA_WIND'))\\\n        .sort(col('USA_WIND').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"COUNTY\", sort=None,title=None), y=\"USA_WIND\")\n    st.altair_chart(c, use_container_width=True)\nwith col2:\n    bar_data = data1.group_by('COUNTY')\\\n        .agg(sum('P0030007').alias('POPULATION_COUNT'))\\\n        .sort(col('POPULATION_COUNT').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"COUNTY\", sort=None,title=None), y=\"POPULATION_COUNT\")\n    st.altair_chart(c, use_container_width=True)\n\nwith col3:\n    bar_data = data1.group_by('COUNTY')\\\n        .agg(sum('P0170022').alias('HOUSING_UNITS'))\\\n        .sort(col('HOUSING_UNITS').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"COUNTY\", sort=None,title=None), y=\"HOUSING_UNITS\")\n    st.altair_chart(c, use_container_width=True)\n\nwith col4:\n    bar_data = data1.group_by('COUNTY')\\\n        .agg(median('P0120047').alias('AGE'))\\\n        .sort(col('AGE').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"COUNTY\", sort=None,title=None), y=\"AGE\")\n    st.altair_chart(c, use_container_width=True)\n\n\n\n# Create data layer for each polygon\ndata_layer = pdk.Layer(\n    \"PolygonLayer\",\n    datapd,\n    opacity=0.3,\n    get_polygon=\"COORDINATES\", \n    filled=True,\n    get_fill_color=\"fill_color\",\n    get_line_color=[0, 0, 0],\n    auto_highlight=True,\n    pickable=True,\n)\n\n# Set the view on the map\nview_state = pdk.ViewState(\n    longitude=LON,\n    latitude=LAT,\n    zoom=6,  # Adjust zoom if needed\n    pitch=0,\n)\n\n\n\n# Render the map with layer and tooltip\nr = pdk.Deck(\n    layers=[data_layer],\n    initial_view_state=view_state,\n    map_style=None,\n    tooltip=tooltip)\n\nst.pydeck_chart(r, use_container_width=True)\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "45be238c-2a76-4143-a3ff-e30fb8b5c30c",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "### Introducing a Filter\n\nHere we are adding a filter by county - we will still render metrics and a map, but this time it is filtered by county and we are seeing the details by population block"
  },
  {
   "cell_type": "code",
   "id": "e265997b-4dfd-4898-aace-e945bcfc0d8c",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import json\nimport streamlit as st\nimport pandas as pd\nimport pydeck as pdk\nimport json\nimport warnings\nimport altair as alt\n\nwarnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\nsession = get_active_session()\nfrom snowflake.snowpark.functions import *\nst.subheader('Affected Populations by the Hurricane')\n### create a filter dropdown using distinct urban extent values\n\ncountyfilter = data.select('COUNTY').distinct().sort('COUNTY')\n\nchoose_county = st.selectbox('Choose County:',countyfilter)\n\ndataf = data.filter(col('COUNTY')==choose_county)\n\n\n\n### create a center point - this time using the centroid method as we are visualising one polygon at a time\ncentre = dataf.with_column('CENTROID',call_function('ST_CENTROID',col('GEO')))\ncentre = centre.with_column('LON',call_function('ST_X',col('CENTROID')))\ncentre = centre.with_column('LAT',call_function('ST_Y',col('CENTROID')))\n\ncentrepd = centre.select('LON','LAT').to_pandas()\nLON = centrepd.LON.iloc[0]\nLAT = centrepd.LAT.iloc[0]\n\n### drop geo as we will be using coordinates field\ndata1 = dataf.drop('GEO')\ndata1 = data1.dropna()\ndatapd = dataf.to_pandas()\n\n### minmax populationss for \nmin_pop = datapd['P0030007'].min()\nmax_pop = datapd['P0030007'].max()\n\n# Define the start and end colors for the gradient (RGBA format)\n# Low population (min_pop) will be BLUE\nCOLOR_LOW_POP = [41, 181, 232, 255] # Blue (R, G, B, A)\n# High population (max_pop) will be ORANGE\nCOLOR_HIGH_POP = [255, 159, 54, 255] # Orange (R, G, B, A)\n\ndef get_color_from_population(population):\n    # Handle the edge case where all population values are the same\n    if min_pop == max_pop:\n        # If no variation, assign a mid-point color (or one of the extremes)\n        return COLOR_HIGH_POP # Or COLOR_LOW_POP, or an average\n    else:\n        # Normalize population to a 0-1 range\n        normalized_pop = (population - min_pop) / (max_pop - min_pop)\n\n        # Interpolate between the start and end colors\n        r = int(COLOR_LOW_POP[0] * (1 - normalized_pop) + COLOR_HIGH_POP[0] * normalized_pop)\n        g = int(COLOR_LOW_POP[1] * (1 - normalized_pop) + COLOR_HIGH_POP[1] * normalized_pop)\n        b = int(COLOR_LOW_POP[2] * (1 - normalized_pop) + COLOR_HIGH_POP[2] * normalized_pop)\n        a = int(COLOR_LOW_POP[3] * (1 - normalized_pop) + COLOR_HIGH_POP[3] * normalized_pop) # Interpolate alpha too if needed, or keep fixed\n\n        return [r, g, b, a]\n\ndatapd['fill_color'] = datapd['P0030007'].apply(get_color_from_population)\n\ntooltip = {\n   \"html\": \"\"\"<b>Name:</b> {NAME} \n                <br> <b>USA Wind:</b> {USA_WIND} \n                <br> <b>Water Surfaces Sq Feet:</b> {AWATER}\n                <br> <b>Populated Density:</b> {P001_CALC_PCTPOPDENSITY}\n                <br> <b>Population County of Housing Units: </b> {P0020003}\n                <br> <b>Population Count: </b> {P0030007}\"\"\",\n   \"style\": {\n       \"width\":\"50%\",\n        \"backgroundColor\": \"steelblue\",\n        \"color\": \"white\",\n       \"text-wrap\": \"balance\"\n   }\n}\n\n\n\n# convert the dataframe to pandas and use a pandas lamda function to extract the coordinates out of each polygon.  \n##pydeck only requires sets of coordinates in arrays, not the polygon itself\n\n\ndatapd[\"COORDINATES\"] = datapd[\"COORDINATES\"].apply(lambda row: json.loads(row))\n\ncol1, col2,col3,col4 = st.columns(4)\nwith col1:\n    bar_data = data1.group_by('NAME')\\\n        .agg(avg('USA_WIND').alias('USA_WIND'))\\\n        .sort(col('USA_WIND').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"NAME\", sort=None,title=None), y=\"USA_WIND\")\n    st.altair_chart(c, use_container_width=True)\nwith col2:\n    bar_data = data1.group_by('NAME')\\\n        .agg(sum('P0030007').alias('POPULATION_COUNT'))\\\n        .sort(col('POPULATION_COUNT').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"NAME\", sort=None,title=None), y=\"POPULATION_COUNT\")\n    st.altair_chart(c, use_container_width=True)\n\nwith col3:\n    bar_data = data1.group_by('NAME')\\\n        .agg(sum('P0170022').alias('HOUSING_UNITS'))\\\n        .sort(col('HOUSING_UNITS').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"NAME\", sort=None,title=None), y=\"HOUSING_UNITS\")\n    st.altair_chart(c, use_container_width=True)\n\nwith col4:\n    bar_data = data1.group_by('NAME')\\\n        .agg(median('P0120047').alias('AGE'))\\\n        .sort(col('AGE').desc()).limit(5).to_pandas()\n        \n    c = alt.Chart(bar_data,height=500).mark_bar(color='#29B5E8')\\\n        .encode(x=alt.X(\"NAME\", sort=None,title=None), y=\"AGE\")\n    st.altair_chart(c, use_container_width=True)\n\n\n\n# Create data layer for each polygon\ndata_layer = pdk.Layer(\n    \"PolygonLayer\",\n    datapd,\n    opacity=0.3,\n    get_polygon=\"COORDINATES\", \n    filled=True,\n    get_fill_color=\"fill_color\",\n    get_line_color=[0, 0, 0],\n    auto_highlight=True,\n    pickable=True,\n)\n\n# Set the view on the map\nview_state = pdk.ViewState(\n    longitude=LON,\n    latitude=LAT,\n    zoom=9,  # Adjust zoom if needed\n    pitch=0,\n)\n\n\n\n# Render the map with layer and tooltip\nr = pdk.Deck(\n    layers=[data_layer],\n    initial_view_state=view_state,\n    map_style=None,\n    tooltip=tooltip)\n\nst.pydeck_chart(r, use_container_width=True)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36cd20d8-ff2e-49aa-9c09-0572446ea066",
   "metadata": {
    "name": "Cortex_insights",
    "collapsed": false
   },
   "source": "## Add cortex to your visualisation\n\nhere we are using the data which we have filtered to prompt cortex to explain what the data is"
  },
  {
   "cell_type": "code",
   "id": "44f872e1-2a22-48ec-ad8e-900c94cfccfc",
   "metadata": {
    "language": "python",
    "name": "what_is_affected_visual",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "cortex_object = data1.drop('GEO_HURRICANE_TRACK','COORDINATES').select(array_agg(object_construct('*')).alias('OBJECT'))\ncortex_object = cortex_object.with_column('prompt',lit('create a report in markdown based on the following dataset'))\n\ninsights = cortex_object.select(call_function('SNOWFLAKE.CORTEX.COMPLETE',\n                                    'claude-4-sonnet',concat('prompt',\n                                                            col('OBJECT').astype(StringType()))))\n\nst.markdown(insights.collect()[0][0])\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "19116e5b-b181-4c15-be8c-7df6e8ce0e0b",
   "metadata": {
    "collapsed": false,
    "name": "head_3"
   },
   "source": [
    "## USING H3\n",
    "\n",
    "you could index the data into H3 hexagons - to do this, you would need to do the following:\n",
    "\n",
    "- Convert each hurricane point to H3.  Here, I have chosen resolution 5\n",
    "- use the coverage function in order to cover each Hurricane Track with the same H3 index resolution.\n",
    "- use the coverage function in order to cover each population block with the same H3 index resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de4420-f538-42ed-9401-281e02658c23",
   "metadata": {
    "language": "python",
    "name": "track_H3"
   },
   "outputs": [],
   "source": [
    "hurricane_pointsH3 = hurricane_points.with_column('H3',call_function('H3_POINT_TO_CELL_STRING',col('GEO'),5))\n",
    "\n",
    "hurricane_pointsH3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7671f0-b759-42af-8022-fb2123eca3cc",
   "metadata": {
    "language": "python",
    "name": "population_h3"
   },
   "outputs": [],
   "source": [
    "H3pop = population.with_column('H3',call_function('H3_TRY_COVERAGE_STRINGS',col('GEO'),5)).with_column('total_indexes',array_size('H3'))\n",
    "H3pop = H3pop.join_table_function('flatten','H3').group_by('VALUE','STATE','COUNTY').agg(min('TOTAL_INDEXES').alias('ind_count'),avg('P0030007').alias('pop_count'))\n",
    "H3pop = H3pop.select(col('VALUE').astype(StringType()).alias('H3'),'STATE','COUNTY',div0('pop_count','ind_count').alias('approx_population'))\n",
    "H3pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7b9f6-ded8-4fed-bade-c653ff56fd7e",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "Next, we will load the H3 tracks and join with the H3 population blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbf498-eb4d-4fef-ac1f-63bd9a99e3d6",
   "metadata": {
    "language": "python",
    "name": "h3_tracks"
   },
   "outputs": [],
   "source": [
    "hurricane_tracks = session.table('HACKATHON_DATASETS.HURRICANE_TRACKS')\n",
    "H3_hurricane_tracks = hurricane_tracks.with_column('H3',call_function('H3_TRY_COVERAGE_STRINGS',col('GEO'),5))\n",
    "H3_hurricane_tracks = H3_hurricane_tracks.join_table_function('flatten','H3').select(col('VALUE').astype(StringType()).alias('H3')).distinct()\n",
    "\n",
    "H3_hurricane_tracks = H3_hurricane_tracks.join(H3pop,'H3')\n",
    "H3_hurricane_tracks = H3_hurricane_tracks\\\n",
    "                .group_by('H3')\\\n",
    "                .agg(avg('APPROX_POPULATION').alias('APPROX_POPULATION'),\n",
    "                     any_value('STATE').alias('STATE'),\n",
    "                     any_value('COUNTY').alias('COUNTY'))\n",
    "H3_hurricane_trackspd = H3_hurricane_tracks.to_pandas()\n",
    "H3_hurricane_trackspd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f096b22-0865-4286-aef6-c09dbff021e7",
   "metadata": {
    "collapsed": false,
    "name": "head_visualise"
   },
   "source": [
    "Finally, you will visualise the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f9c9d-1ba7-43a8-abcb-c58099bd8a6d",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "visualise_results"
   },
   "outputs": [],
   "source": [
    "min_pop = H3_hurricane_trackspd['APPROX_POPULATION'].min()\n",
    "max_pop = H3_hurricane_trackspd['APPROX_POPULATION'].max()\n",
    "\n",
    "# Define the start and end colors for the gradient (RGBA format)\n",
    "# Low population (min_pop) will be BLUE\n",
    "COLOR_LOW_POP = [41, 181, 232, 255] # Blue (R, G, B, A)\n",
    "# High population (max_pop) will be ORANGE\n",
    "COLOR_HIGH_POP = [255, 159, 54, 255] # Orange (R, G, B, A)\n",
    "\n",
    "def get_color_from_population(population):\n",
    "    # Handle the edge case where all population values are the same\n",
    "    if min_pop == max_pop:\n",
    "        # If no variation, assign a mid-point color (or one of the extremes)\n",
    "        return COLOR_HIGH_POP # Or COLOR_LOW_POP, or an average\n",
    "    else:\n",
    "        # Normalize population to a 0-1 range\n",
    "        normalized_pop = (population - min_pop) / (max_pop - min_pop)\n",
    "\n",
    "        # Interpolate between the start and end colors\n",
    "        r = int(COLOR_LOW_POP[0] * (1 - normalized_pop) + COLOR_HIGH_POP[0] * normalized_pop)\n",
    "        g = int(COLOR_LOW_POP[1] * (1 - normalized_pop) + COLOR_HIGH_POP[1] * normalized_pop)\n",
    "        b = int(COLOR_LOW_POP[2] * (1 - normalized_pop) + COLOR_HIGH_POP[2] * normalized_pop)\n",
    "        a = int(COLOR_LOW_POP[3] * (1 - normalized_pop) + COLOR_HIGH_POP[3] * normalized_pop) # Interpolate alpha too if needed, or keep fixed\n",
    "\n",
    "        return [r, g, b, a]\n",
    "\n",
    "H3_hurricane_trackspd['fill_color'] = H3_hurricane_trackspd['APPROX_POPULATION'].apply(get_color_from_population)\n",
    "\n",
    "hurricane_pointsH3pd = hurricane_pointsH3.to_pandas()\n",
    "\n",
    "h3points = pdk.Layer(\n",
    "        \"H3HexagonLayer\",\n",
    "        hurricane_pointsH3pd,\n",
    "        pickable=False,\n",
    "        stroked=True,\n",
    "        filled=False,\n",
    "        extruded=False,\n",
    "        get_hexagon=\"H3\",\n",
    "        get_line_color=[0,0,0],\n",
    "        line_width_min_pixels=2,\n",
    "        opacity=0.4)\n",
    "\n",
    "h3 = pdk.Layer(\n",
    "        \"H3HexagonLayer\",\n",
    "        H3_hurricane_trackspd,\n",
    "        pickable=True,\n",
    "        stroked=True,\n",
    "        filled=True,\n",
    "        extruded=True,\n",
    "        get_hexagon=\"H3\",\n",
    "        get_fill_color=\"fill_color\",\n",
    "        line_width_min_pixels=0,\n",
    "        opacity=0.4)\n",
    "\n",
    "tooltip = {\n",
    "   \"html\": \"\"\"<b>H3:</b> {H3} <br> <b>Approx Population:</b> {APPROX_POPULATION}\"\"\",\n",
    "   \"style\": {\n",
    "       \"width\":\"50%\",\n",
    "        \"backgroundColor\": \"steelblue\",\n",
    "        \"color\": \"white\",\n",
    "       \"text-wrap\": \"balance\"\n",
    "   }\n",
    "}\n",
    "\n",
    "st.pydeck_chart(pdk.Deck(\n",
    "    map_style=None,\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=600\n",
    "        ),\n",
    "    \n",
    "layers= [h3,h3points], tooltip = tooltip\n",
    "\n",
    "))"
   ]
  }
 ]
}